{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark -Linear Regression model for House Price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set environment\n",
    "import os\n",
    "import sys\n",
    " \n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Sparksession driver\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Classification of Car Dataset\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocecced data in sklearn was submitted to a csv file and that is imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+-------------------------+----------------------------+---------------+-----------+\n",
      "|Avg. Area Income|Avg. Area House Age|Avg. Area Number of Rooms|Avg. Area Number of Bedrooms|Area Population|      Price|\n",
      "+----------------+-------------------+-------------------------+----------------------------+---------------+-----------+\n",
      "|     79545.45857|        5.682861322|              7.009188143|                        4.09|     23086.8005|1059033.558|\n",
      "|     79248.64245|        6.002899808|              6.730821019|                        3.09|    40173.07217|1505890.915|\n",
      "|     61287.06718|         5.86588984|               8.51272743|                        5.13|     36882.1594|1058987.988|\n",
      "+----------------+-------------------+-------------------------+----------------------------+---------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('new_regression2_try.csv',inferSchema=True,header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Avg. Area Income: double (nullable = true)\n",
      " |-- Avg. Area House Age: double (nullable = true)\n",
      " |-- Avg. Area Number of Rooms: double (nullable = true)\n",
      " |-- Avg. Area Number of Bedrooms: double (nullable = true)\n",
      " |-- Area Population: double (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL like schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns\n",
    "df = df.toDF('Avg_Income', 'Avg_House_Age', 'Avg_No_of_Rooms', 'Avg_No_of_Bedrooms', 'Population', 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of cells in column Avg_Income with null values: 0\n",
      "no. of cells in column Avg_House_Age with null values: 0\n",
      "no. of cells in column Avg_No_of_Rooms with null values: 0\n",
      "no. of cells in column Avg_No_of_Bedrooms with null values: 0\n",
      "no. of cells in column Population with null values: 0\n",
      "no. of cells in column price with null values: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "for col in df.columns:\n",
    "    print(\"no. of cells in column\", col, \"with null values:\", df.filter(df[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|      Price|\n",
      "+--------------------+-----------+\n",
      "|[79545.45857,5.68...|1059033.558|\n",
      "|[79248.64245,6.00...|1505890.915|\n",
      "|[61287.06718,5.86...|1058987.988|\n",
      "|[63345.24005,7.18...|1260616.807|\n",
      "|[59982.19723,5.04...|630943.4893|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----------+\n",
      "|            features|      Price|\n",
      "+--------------------+-----------+\n",
      "|[79545.45857,5.68...|1059033.558|\n",
      "|[79248.64245,6.00...|1505890.915|\n",
      "|[61287.06718,5.86...|1058987.988|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all the independent variables need to be packed into one column of vector type\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Avg_Income\",\"Avg_House_Age\",\"Avg_No_of_Rooms\",\"Avg_No_of_Bedrooms\",\"Population\"], \n",
    "                            outputCol=\"features\")\n",
    "feature_vec=assembler.transform(df).select('features','Price')\n",
    "feature_vec.show(5)\n",
    "df = feature_vec.select(['features','Price'])\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard scaling for data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scaler_model.transform(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PCA \n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=5, inputCol='features', outputCol='features_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pca model on feature vector\n",
    "pca_model = pca.fit(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        features_pca|\n",
      "+--------------------+\n",
      "|[-76466.360363686...|\n",
      "|[-74261.531550111...|\n",
      "|[-56780.370568401...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca_data = pca_model.transform(feature_vec).select('features_pca')\n",
    "pca_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|      Price|\n",
      "+--------------------+-----------+\n",
      "|[79545.45857,5.68...|1059033.558|\n",
      "|[79248.64245,6.00...|1505890.915|\n",
      "|[61287.06718,5.86...|1058987.988|\n",
      "|[63345.24005,7.18...|1260616.807|\n",
      "|[59982.19723,5.04...|630943.4893|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concatenating label column to feature_pca columns\n",
    "from pyspark.sql.functions import concat\n",
    "from pyspark.sql.functions import col\n",
    "res = df.withColumn(\"Price\",concat(col(\"Price\")))\n",
    "res.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|            features|      Price|        features_pca|\n",
      "+--------------------+-----------+--------------------+\n",
      "|[79545.45857,5.68...|1059033.558|[-76466.360363686...|\n",
      "|[79248.64245,6.00...|1505890.915|[-74261.531550111...|\n",
      "|[61287.06718,5.86...|1058987.988|[-56780.370568401...|\n",
      "|[63345.24005,7.18...|1260616.807|[-59113.129456245...|\n",
      "|[59982.19723,5.04...|630943.4893|[-56660.484529818...|\n",
      "|[80175.75416,4.98...|1068138.074|[-76683.415807619...|\n",
      "|[64698.46343,6.02...|1502055.817|[-57493.737275412...|\n",
      "|[78394.33928,6.98...|1573936.564|[-73821.322982044...|\n",
      "|[59927.66081,5.36...|798869.5328|[-56267.234551529...|\n",
      "|[81885.92718,4.42...|1545154.813|[-76884.871691391...|\n",
      "|[80527.47208,8.09...|1707045.722|[-74744.166707544...|\n",
      "|[50593.6955,4.496...|663732.3969|[-46437.724134040...|\n",
      "|[39033.80924,7.67...|1042814.098|[-34405.209757966...|\n",
      "|[73163.66344,6.91...|1291331.518|[-69091.803888298...|\n",
      "|[69391.38018,5.34...| 1402818.21|[-64986.010383657...|\n",
      "|[73091.86675,5.44...| 1306674.66|[-69959.013914904...|\n",
      "|[79706.96306,5.06...|  1556786.6|[-74767.867933992...|\n",
      "|[61929.07702,4.78...|528485.2467|[-58791.692909075...|\n",
      "|[63508.1943,5.947...|1019425.937|[-59117.521159226...|\n",
      "|[62085.2764,5.739...|1030591.429|[-56674.886676357...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "df = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "pca_data = pca_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "result_df = df.join(pca_data, (\"row_id\")).drop(\"row_id\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      Price|        features_pca|\n",
      "+-----------+--------------------+\n",
      "|1059033.558|[-76466.360363686...|\n",
      "|1505890.915|[-74261.531550111...|\n",
      "|1058987.988|[-56780.370568401...|\n",
      "|1260616.807|[-59113.129456245...|\n",
      "|630943.4893|[-56660.484529818...|\n",
      "+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping insignificant features\n",
    "result_df = result_df.drop(\"features\")\n",
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      Price|        features_pca|\n",
      "+-----------+--------------------+\n",
      "|31140.51762|[-34014.637469225...|\n",
      "|88591.77016|[-57911.475322092...|\n",
      "|151527.0826|[-46230.447778747...|\n",
      "+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = result_df.randomSplit([.75,.25],seed=0)\n",
    "train_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- features_pca: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "Coefficients: [-19.626514833787713,17.214897309177925,-65817.10532679415,-161436.90740462617,107241.46474226534]\n",
      "Intercept: -2628693.7946493393\n"
     ]
    }
   ],
   "source": [
    "#import linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create initial LinearRegression model\n",
    "lr = LinearRegression(labelCol=\"Price\", featuresCol=\"features_pca\",  \n",
    "                        maxIter=10, regParam=0.0001,  \n",
    "                        elasticNetParam=0.0)\n",
    "#fit linear regression model on training data\n",
    "lrModel = lr.fit(train_data)\n",
    "predictions = lrModel.transform(test_data)\n",
    "predictions.printSchema()\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = feature_vec.randomSplit([.75,.25],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = feature_vec.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [21.55934064044135,165622.22550399075,120450.23504599131,3116.88534896747,15.264745652303285]\n",
      "Intercept: -2644194.4727492738\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Price', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 101313.892997\n",
      "r2: 0.918205\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Metrics R2 & RMSE\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERPRETATION\n",
    "\n",
    "R2Score is 0.91 which shows our model is a very good fit for the data it is a decent enough model to predict the independ variable\n",
    "\n",
    "This score is very similar to the score that we got in Skleran Linear Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
